{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": "import pandas as pd",
   "id": "10b19e6025b8c5d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sources and Citations\n",
    "\n",
    "Fisman, R. J., Iyengar, S. S, Kamenica, E. & Simonson, I. (2006). *Gender Differences in Mate Selection: Evidence from a Speed Dating Experiment*. [http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating]. The Quarterly Journal of Economics. https://academiccommons.columbia.edu/doi/10.7916/D8FB585Z\n",
    "\n",
    "Short post and supplemental information about the dataset and the associated experiment: [https://statmodeling.stat.columbia.edu/2008/01/21/the_speeddating_1](https://statmodeling.stat.columbia.edu/2008/01/21/the_speeddating_1). \n",
    "\n",
    "The link for the file share in the formal citation at the start of this cell has both the data in CSV format and a key for understanding the CSV: [http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating](http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating)\n",
    "\n",
    "Dataset that I originally found and that led me to the source at the Columbia site: [https://www.openml.org/search?type=data&sort=runs&status=active&id=40536](https://www.openml.org/search?type=data&sort=runs&status=active&id=40536)"
   ],
   "id": "bf0a1369558ebc74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data and Research Paper Summary \n",
    "\n",
    "This dataset comes from an experiment by Raymond Fisman and Sheena Iyengar of the Columbia Business School about participants in a speed dating experiment. I found an open copy of the data hosted on a Columbia University file share from a well-known statistics professor at Columbia.\n",
    "\n",
    "The researchers conducted a series of separate speed date events where they rotated participants through four-minute speed dates and asked participants to rate their partners after each date. The ratings are based around five attributes:\n",
    "\n",
    "- Attractive\n",
    "- Sincere\n",
    "- Intelligent\n",
    "- Fun\n",
    "- Ambitious\n",
    "- Shared interests\n",
    "\n",
    "The researchers end up using attractiveness, intelligence, and ambition while dropping the remaining three attributes. Participants are also asked to fill out pre-event surveys to provide non-event data that is also used to determine degrees of similarity between participants. \n",
    "\n",
    "Part of the large feature space in the CSV is due to the combinations of attribute ratings across different partners, averages about individuals based on ratings by all of their partners, and other ways of combining the event survey and pre-event survey results.\n",
    "\n",
    "The researchers focus on differential gender preferences about these attributes, trying to glean ways in which women and men value different characteristics during these dates. \n",
    "\n",
    "It is important to note a couple of potential issues with the experiment design. The participants were students in Columbia University graduate and professional programs, and the dates are heteronormative. These both restrict the generalizability of the findings. I am also curious about the justification of using speed dating as a stand-in for all dating since speed dating seems to me to be a very different experience from dating over a longer time scale.\n",
    "\n",
    "The data explores how different demographics and personal characteristics affect participants' feelings about the speed dates they participate in. One target variable is the `match` column that indicates if both participants in the speed date want to meet again, but the experimental design allows for decisions that are more granular than match or no match, and the associated paper focuses on the `dec` column that has yes and no decisions by individuals about each date instead of matches that have yes decisions on both sides. The researchers use the `dec` values to determine what women and men value in partners, honing in on differences in what women and men prioritize.\n",
    "\n",
    "The paper also references different social theories and starts to explore larger meanings about men and women based on this research's findings, including starting to challenge some of their own findings."
   ],
   "id": "31f471e141b1a706"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Project\n",
    "\n",
    "The choice of marriage partner is a perennial question. The more successful relationships out there, the better. On a personal note, I am looking at starting dating again myself, so I am curious about research that can help me to understand my own preferences and how those may fit in with dates that I go on. \n",
    "\n",
    "Because of the size of the feature space, layers of complexity, social theory, and prior knowledge built into the approach in the paper, I am going to attempt something simpler: can I learn what factors on a date will influence me to lean towards wanting to meet for a second date? \n",
    "\n",
    "This is a classification problem that will use different features to predict if I will make a yes or no decision at the end of the date. This is useful in terms of helping me to make sense what might be general signals that people tend to pick up on when making this decision, and I can then reflect on if I want to settle for that more automatic decision or if I might be missing something that could affect that decision.\n",
    "\n",
    "This also does not tell me if the other person will choose yes. A next step of this analysis would be to begin to look at features leading to matches, but I would rather break the progression to that up into steps. Also, the paper mentions a lot of complexity and social theory that I would like to read about before pivoting to the more targeted match decision.\n"
   ],
   "id": "81b2aeb68c1ecb50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fixing Bad Characters in Source CSV\n",
    "\n",
    "The source CSV from the link above has some characters that got encoded with the Unicode replacement character, the diamond with a question mark in it. You can search for this by using a regex and looking for code `\\uFFFD` or `\\xEF\\xBF\\xBD`. These replacement characters look like the only non-ASCII characters. You can search for non-ASCII characters with the character set `[^\\x00-\\x7F]`. A visual scan of the bad rows looks like the characters are meant to be 'é'.\n",
    "\n",
    "Sample bad value in the `undergra` field from the source CSV: Ecole Normale Sup�rieure, Paris\n",
    "\n",
    "\n",
    "The following code block calls out to the shell to create a copy of the speed dating CSV and then run `sed` to replace the replacement characters with regular 'e's to keep everything in ASCII. I decided to use shell commands because I had trouble finding a way to read the bad characters in with Python, so I could not get to the step of replacing them in Python. I was able to manually change them with find and replace in Vim, so I modified that for `sed` for the solution below.\n",
    "\n",
    "I ran this on MacOS. You may need to make some tweaks for Windows in particular."
   ],
   "id": "ff276e635ece96ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "! cp ./data/Speed\\ Dating\\ Data.csv ./data/speed_dating_data_fixed.csv\n",
    "! sed -i '' 's/\\xEF\\xBF\\xBD/e/g' ./data/speed_dating_data_fixed.csv  "
   ],
   "id": "38ce16dd610a88d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading and Initial Exploration\n",
    "\n",
    "The research paper goes into detail about what features and combinations of features are in the dataset and primarily looks at differential gender effects between male and female participants.\n",
    "\n",
    "The main target feature is `dec` for decision and has 1/0 boolean values for yes/no in terms of if that individual wants to see the other person again. A secondary target feature could be `match` which indicates if both participants want to see each other again. The paper goes the route of focusing on differences between genders for `dec`.\n",
    "\n"
   ],
   "id": "f50dd43b59522039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T18:00:11.832234Z",
     "start_time": "2024-05-04T18:00:11.573764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The low_memory=False param tells pandas to determine column data types by looking at all rows\n",
    "# in each column, resulting in needing to read in the entire CSV before being able to determine\n",
    "# data types. low_memory=True results in chunking when reading in the file, and each chunk can\n",
    "# infer a different data type. I will likely pass in a stricter data type spec later on to keep\n",
    "# chunking but to get explicit data types\n",
    "\n",
    "df_raw = pd.read_csv('./data/speed_dating_data_fixed.csv', low_memory=False)\n",
    "df_raw.head(10)"
   ],
   "id": "fffe6b33320eb605",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   iid   id  gender  idg  condtn  wave round position  positin1  order  ...  \\\n",
       "0  1.0  1.0     0.0    1       1     1    10        7       NaN      4  ...   \n",
       "1  1.0  1.0     0.0    1       1     1    10        7       NaN      3  ...   \n",
       "2  1.0  1.0     0.0    1       1     1    10        7       NaN     10  ...   \n",
       "3  1.0  1.0     0.0    1       1     1    10        7       NaN      5  ...   \n",
       "4  1.0  1.0     0.0    1       1     1    10        7       NaN      7  ...   \n",
       "5  1.0  1.0     0.0    1       1     1    10        7       NaN      6  ...   \n",
       "6  1.0  1.0     0.0    1       1     1    10        7       NaN      1  ...   \n",
       "7  1.0  1.0     0.0    1       1     1    10        7       NaN      2  ...   \n",
       "8  1.0  1.0     0.0    1       1     1    10        7       NaN      8  ...   \n",
       "9  1.0  1.0     0.0    1       1     1    10        7       NaN      9  ...   \n",
       "\n",
       "   attr3_3  sinc3_3 intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  intel5_3  \\\n",
       "0      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "1      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "2      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "3      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "4      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "5      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "6      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "7      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "8      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "9      5.0      7.0      7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "\n",
       "   fun5_3  amb5_3  \n",
       "0     NaN     NaN  \n",
       "1     NaN     NaN  \n",
       "2     NaN     NaN  \n",
       "3     NaN     NaN  \n",
       "4     NaN     NaN  \n",
       "5     NaN     NaN  \n",
       "6     NaN     NaN  \n",
       "7     NaN     NaN  \n",
       "8     NaN     NaN  \n",
       "9     NaN     NaN  \n",
       "\n",
       "[10 rows x 195 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 195 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T17:07:05.518235Z",
     "start_time": "2024-05-04T17:07:05.485281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'The dataset has {df_raw.shape[0]:,} rows and {df_raw.shape[1]} columns')\n",
    "df_raw.dtypes"
   ],
   "id": "c72a723257d77333",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 8,379 rows and 195 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iid         float64\n",
       "id          float64\n",
       "gender      float64\n",
       "idg           int64\n",
       "condtn        int64\n",
       "             ...   \n",
       "attr5_3     float64\n",
       "sinc5_3     float64\n",
       "intel5_3    float64\n",
       "fun5_3      float64\n",
       "amb5_3      float64\n",
       "Length: 195, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-Cleaning Exploratory Data Analysis (EDA)\n",
   "id": "231a79e8d517bc51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "The first goal will be to reduce the feature space since the CSV has a large number of features. The paper helps in terms of understanding where all these columns are coming from and how they are contributing to the study. The feature reduction will take a few different approaches. First will be to perform some exploratory data analysis (EDA) to see if I can manually remove some features based on the EDA findings combined with domain knowledge gleaned from the paper. Another approach will be to limit features based on domain knowledge and then test out simplification methods such as principal components analysis (PCA) and ridge or lasso regression.\n",
    "\n",
    "After that, we will need to determine how to handle missing values as well as when and how to substitute in level values for factor features.\n"
   ],
   "id": "2f568e1865ce5723"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## EDA",
   "id": "22855a998ca88ec1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Identification\n",
    "\n",
    "I also want to test out a few different classification models. The obvious model for a binary classification problem is logistic regression. I also want to test out support vector machines (SVM), especially for a larger feature space. Finally, I would like to test out a tree-based model, but the specifics for this one will depend on what I find from some of the feature reduction.\n"
   ],
   "id": "c7a59318786d5255"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Building",
   "id": "ad11384b560962b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "d79c68610534c8fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results",
   "id": "5782bd27c7603776"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Conclusion",
   "id": "86959f6e581f4fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
